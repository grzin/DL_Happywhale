{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "from scipy import spatial\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import dot, sqrt\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display_html\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "from albumentations.pytorch import transforms\n",
    "import albumentations\n",
    "import timm\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Environment check\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device available now: cuda\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed = 1234):\n",
    "    '''\n",
    "    üå±src:https://www.kaggle.com/andradaolteanu/melanoma-competiton-aug-resnet-effnet-lb-0-91\n",
    "    Sets the seed of the entire notebook so results are the same every time we run.\n",
    "    This is for REPRODUCIBILITY.'''\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "set_seed()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device available now:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --------- INITIAL PARAMETERS ---------\n",
    "TRAIN_FOLDER = \"./happy-whale-and-dolphin/aug_train/\"\n",
    "TEST_FOLDER = \"./happy-whale-and-dolphin/test_images/\"\n",
    "\n",
    "# Set some parameters for sanity checks & experimenting\n",
    "N_SPLITS = 5\n",
    "BATCH_SIZE = 16\n",
    "MODEL_NAME = 'efficientnet_b0'\n",
    "NUM_CLASSES = 15587\n",
    "NO_NEURONS = 250\n",
    "EMBEDDING_SIZE = 128\n",
    "# -------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>species</th>\n",
       "      <th>individual_id</th>\n",
       "      <th>file_path</th>\n",
       "      <th>class</th>\n",
       "      <th>count</th>\n",
       "      <th>individual_key</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00021adfb725ed.jpg</td>\n",
       "      <td>melon_headed_whale</td>\n",
       "      <td>cadddb1636b9</td>\n",
       "      <td>./happy-whale-and-dolphin/aug_train/00021adfb7...</td>\n",
       "      <td>whale</td>\n",
       "      <td>5</td>\n",
       "      <td>12348</td>\n",
       "      <td>./happy-whale-and-dolphin/aug_train/00021adfb7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00021adfb725ed_0.jpg</td>\n",
       "      <td>melon_headed_whale</td>\n",
       "      <td>cadddb1636b9</td>\n",
       "      <td>./happy-whale-and-dolphin/aug_train/00021adfb7...</td>\n",
       "      <td>whale</td>\n",
       "      <td>5</td>\n",
       "      <td>12348</td>\n",
       "      <td>./happy-whale-and-dolphin/aug_train/00021adfb7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00021adfb725ed_1.jpg</td>\n",
       "      <td>melon_headed_whale</td>\n",
       "      <td>cadddb1636b9</td>\n",
       "      <td>./happy-whale-and-dolphin/aug_train/00021adfb7...</td>\n",
       "      <td>whale</td>\n",
       "      <td>5</td>\n",
       "      <td>12348</td>\n",
       "      <td>./happy-whale-and-dolphin/aug_train/00021adfb7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00021adfb725ed_2.jpg</td>\n",
       "      <td>melon_headed_whale</td>\n",
       "      <td>cadddb1636b9</td>\n",
       "      <td>./happy-whale-and-dolphin/aug_train/00021adfb7...</td>\n",
       "      <td>whale</td>\n",
       "      <td>5</td>\n",
       "      <td>12348</td>\n",
       "      <td>./happy-whale-and-dolphin/aug_train/00021adfb7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00021adfb725ed_3.jpg</td>\n",
       "      <td>melon_headed_whale</td>\n",
       "      <td>cadddb1636b9</td>\n",
       "      <td>./happy-whale-and-dolphin/aug_train/00021adfb7...</td>\n",
       "      <td>whale</td>\n",
       "      <td>5</td>\n",
       "      <td>12348</td>\n",
       "      <td>./happy-whale-and-dolphin/aug_train/00021adfb7...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TEST:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>species</th>\n",
       "      <th>individual_id</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00021adfb725ed.jpg</td>\n",
       "      <td>melon_headed_whale</td>\n",
       "      <td>cadddb1636b9</td>\n",
       "      <td>./happy-whale-and-dolphin/test_images/00021adf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000562241d384d.jpg</td>\n",
       "      <td>humpback_whale</td>\n",
       "      <td>1a71fbb72250</td>\n",
       "      <td>./happy-whale-and-dolphin/test_images/00056224...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0007c33415ce37.jpg</td>\n",
       "      <td>false_killer_whale</td>\n",
       "      <td>60008f293a2b</td>\n",
       "      <td>./happy-whale-and-dolphin/test_images/0007c334...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0007d9bca26a99.jpg</td>\n",
       "      <td>bottlenose_dolphin</td>\n",
       "      <td>4b00fe572063</td>\n",
       "      <td>./happy-whale-and-dolphin/test_images/0007d9bc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00087baf5cef7a.jpg</td>\n",
       "      <td>humpback_whale</td>\n",
       "      <td>8e5253662392</td>\n",
       "      <td>./happy-whale-and-dolphin/test_images/00087baf...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Import the data\n",
    "train = pd.read_csv(\"./happy-whale-and-dolphin/aug_train.csv\")\n",
    "test = pd.read_csv(\"./sample_submission.csv\")\n",
    "train['individual_key'] = train.groupby(['individual_id']).ngroup()\n",
    "\n",
    "# Update path to new image folders\n",
    "train[\"path\"] = TRAIN_FOLDER + train[\"image\"]\n",
    "test[\"path\"] = TEST_FOLDER + test[\"image\"]\n",
    "\n",
    "print(\"TRAIN:\")\n",
    "display_html(train.head())\n",
    "print(\"\\n\", \"TEST:\")\n",
    "display_html(test.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HappyWhaleDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, csv, trainFlag):\n",
    "        '''Module to create the PyTorch Dataset.\n",
    "        csv: full dataframe (train or test)\n",
    "        trainFlag: True if csv is a training/validation dataset, False otherwise\n",
    "        return: image and class target if trainFlag, otherwise only image'''\n",
    "        \n",
    "        self.csv = csv\n",
    "        self.trainFlag = trainFlag\n",
    "        if self.trainFlag:\n",
    "            self.transform = albumentations.Compose([\n",
    "                albumentations.Resize(128, 128),\n",
    "                albumentations.HorizontalFlip(),\n",
    "                albumentations.VerticalFlip(),\n",
    "                albumentations.Rotate(),\n",
    "                albumentations.Normalize(),\n",
    "                # B&W?\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = albumentations.Compose([\n",
    "                albumentations.Normalize()\n",
    "            ])\n",
    "\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.csv.shape[0]\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Get data\n",
    "        row = self.csv.iloc[index]\n",
    "        \n",
    "        # Read and transform the image\n",
    "        image = cv2.imread(row.path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        transformed_img = self.transform(image=image)['image'].astype(np.float32)\n",
    "        image = transformed_img.transpose(2, 0, 1)\n",
    "        image = torch.tensor(image)            \n",
    "\n",
    "        if self.trainFlag:\n",
    "            # Retrieve the target group\n",
    "            target = torch.tensor(row.individual_key)\n",
    "            return image, target\n",
    "        \n",
    "        else:\n",
    "            return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Batch 0 ---\n",
      "Image Shape: torch.Size([3, 3, 128, 128])\n",
      "Target: tensor([12348, 12348, 12348]) \n",
      "\n",
      "--- Batch 1 ---\n",
      "Image Shape: torch.Size([3, 3, 128, 128])\n",
      "Target: tensor([12348, 12348,  1636]) \n",
      "\n",
      "--- Batch 2 ---\n",
      "Image Shape: torch.Size([3, 3, 128, 128])\n",
      "Target: tensor([1636, 1636, 1636]) \n",
      "\n",
      "--- Batch 3 ---\n",
      "Image Shape: torch.Size([3, 3, 128, 128])\n",
      "Target: tensor([5842, 4551, 8721]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Example for the Dataset data\n",
    "example_dataset = HappyWhaleDataset(train.head(12), trainFlag=True)\n",
    "example_loader = DataLoader(example_dataset, batch_size=3)\n",
    "\n",
    "for k, (image, target) in enumerate(example_loader):\n",
    "    print(f\"--- Batch {k} ---\")\n",
    "    print(\"Image Shape:\", image.shape)\n",
    "    print(\"Target:\", target, \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# src: https://amaarora.github.io/2020/08/30/gempool.html\n",
    "\n",
    "class GeM(nn.Module):\n",
    "    def __init__(self, p=3, eps=1e-6):\n",
    "        super(GeM,self).__init__()\n",
    "        self.p = nn.Parameter(torch.ones(1)*p)\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gem(x, p=self.p, eps=self.eps)\n",
    "        \n",
    "    def gem(self, x, p=3, eps=1e-6):\n",
    "        # Applies 2D average-pooling operation in kH * kW regions by step size\n",
    "        return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + ', ' + 'eps=' + str(self.eps) + ')'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcMarginProduct(nn.Module):\n",
    "    def __init__(self, in_features, out_features, s=30.0, \n",
    "                 m=0.50, easy_margin=False, ls_eps=0.0):\n",
    "        '''\n",
    "        in_features: dimension of the input\n",
    "        out_features: dimension of the last layer (in our case the classification)\n",
    "        s: norm of input feature\n",
    "        m: margin\n",
    "        ls_eps: label smoothing'''\n",
    "        \n",
    "        super(ArcMarginProduct, self).__init__()\n",
    "        self.in_features, self.out_features = in_features, out_features\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.ls_eps = ls_eps\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        # Fills the input `Tensor` with values according to the method described in\n",
    "        # `Understanding the difficulty of training deep feedforward neural networks`\n",
    "        # Glorot, X. & Bengio, Y. (2010)\n",
    "        # using a uniform distribution.\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m, self.sin_m = math.cos(m), math.sin(m)\n",
    "        self.th = math.cos(math.pi - m)\n",
    "        self.mm = math.sin(math.pi - m) * m\n",
    "\n",
    "    def forward(self, input, label):\n",
    "        # --------------------------- cos(theta) & phi(theta) ---------------------\n",
    "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
    "        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        if self.easy_margin:\n",
    "            phi = torch.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
    "        # --------------------------- convert label to one-hot ---------------------\n",
    "        one_hot = torch.zeros(cosine.size()).to(device)\n",
    "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
    "        if self.ls_eps > 0:\n",
    "            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n",
    "        # -------------torch.where(out_i = {x_i if condition_i else y_i) ------------\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "        output *= self.s\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HappyWhaleModel(nn.Module):\n",
    "    def __init__(self, modelName, numClasses, noNeurons, embeddingSize):\n",
    "        \n",
    "        super(HappyWhaleModel, self).__init__()\n",
    "        # Retrieve pretrained weights\n",
    "        self.backbone = timm.create_model(modelName, pretrained=True)\n",
    "        # Save the number features from the backbone\n",
    "        ### different models have different numbers e.g. EffnetB3 has 1536\n",
    "        backbone_features = self.backbone.classifier.in_features\n",
    "        self.backbone.classifier = nn.Identity() # ?????\n",
    "        self.backbone.global_pool = nn.Identity() # ?????\n",
    "        self.gem = GeM()\n",
    "        # Embedding layer (what we actually need)\n",
    "        self.embedding = nn.Sequential(nn.Linear(backbone_features, noNeurons),\n",
    "                                       nn.BatchNorm1d(noNeurons),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Dropout(p=0.2),\n",
    "                                       \n",
    "                                       nn.Linear(noNeurons, embeddingSize),\n",
    "                                       nn.BatchNorm1d(embeddingSize),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Dropout(p=0.2))\n",
    "        self.arcface = ArcMarginProduct(in_features=embeddingSize, \n",
    "                                        out_features=numClasses,\n",
    "                                        s=30.0, m=0.50, easy_margin=False, ls_eps=0.0)\n",
    "        \n",
    "        \n",
    "    def forward(self, image, target=None, prints=False):\n",
    "        '''If there is a target it means that the model is training on the dataset.\n",
    "        If there is no target, that means the model is predicting on the test dataset.\n",
    "        In this case we would skip the ArcFace layer and return only the image embeddings.\n",
    "        '''\n",
    "        \n",
    "        features = self.backbone(image)\n",
    "        # flatten transforms from e.g.: [3, 1536, 1, 1] to [3, 1536]\n",
    "        gem_pool = self.gem(features).flatten(1)\n",
    "        embedding = self.embedding(gem_pool)\n",
    "        if target != None:\n",
    "            out = self.arcface(embedding, target)\n",
    "        \n",
    "        if prints:\n",
    "            print(\"0. IN:\", \"image shape:\", image.shape, \"target:\", target)\n",
    "            print(\"1. Backbone Output:\", features.shape)\n",
    "            print(\"2. GeM Pool Output:\", gem_pool.shape)\n",
    "            print(\"3. Embedding Output:\", embedding.shape)\n",
    "            if target != None:\n",
    "                print(\"4. ArcFace Output:\", out.shape)\n",
    "        \n",
    "        if target != None:\n",
    "            return out, embedding\n",
    "        else:\n",
    "            return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/efficientnet_b0.ra_in1k)\n",
      "INFO:timm.models._hub:[timm/efficientnet_b0.ra_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Create an example model - Effnet\n",
    "model_example = HappyWhaleModel(MODEL_NAME, NUM_CLASSES, NO_NEURONS, EMBEDDING_SIZE).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Batch 0 ===\n",
      "0. IN: image shape: torch.Size([3, 3, 128, 128]) target: tensor([12348, 12348, 12348], device='cuda:0')\n",
      "1. Backbone Output: torch.Size([3, 1280, 4, 4])\n",
      "2. GeM Pool Output: torch.Size([3, 1280])\n",
      "3. Embedding Output: torch.Size([3, 128])\n",
      "4. ArcFace Output: torch.Size([3, 15587])\n",
      "--- LOSS --- 28.241270065307617 \n",
      "\n",
      "=== Batch 1 ===\n",
      "0. IN: image shape: torch.Size([3, 3, 128, 128]) target: tensor([12348, 12348,  1636], device='cuda:0')\n",
      "1. Backbone Output: torch.Size([3, 1280, 4, 4])\n",
      "2. GeM Pool Output: torch.Size([3, 1280])\n",
      "3. Embedding Output: torch.Size([3, 128])\n",
      "4. ArcFace Output: torch.Size([3, 15587])\n",
      "--- LOSS --- 26.934799194335938 \n",
      "\n",
      "=== Batch 2 ===\n",
      "0. IN: image shape: torch.Size([3, 3, 128, 128]) target: tensor([1636, 1636, 1636], device='cuda:0')\n",
      "1. Backbone Output: torch.Size([3, 1280, 4, 4])\n",
      "2. GeM Pool Output: torch.Size([3, 1280])\n",
      "3. Embedding Output: torch.Size([3, 128])\n",
      "4. ArcFace Output: torch.Size([3, 15587])\n",
      "--- LOSS --- 27.42755126953125 \n",
      "\n",
      "=== Batch 3 ===\n",
      "0. IN: image shape: torch.Size([3, 3, 128, 128]) target: tensor([5842, 4551, 8721], device='cuda:0')\n",
      "1. Backbone Output: torch.Size([3, 1280, 4, 4])\n",
      "2. GeM Pool Output: torch.Size([3, 1280])\n",
      "3. Embedding Output: torch.Size([3, 128])\n",
      "4. ArcFace Output: torch.Size([3, 15587])\n",
      "--- LOSS --- 27.512643814086914 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Criterion\n",
    "criterion_example = nn.CrossEntropyLoss()\n",
    "\n",
    "# We'll use previous datasets & dataloader\n",
    "for k, (image, target) in enumerate(example_loader):\n",
    "    print(f\"=== Batch {k} ===\")\n",
    "    image, target = image.to(device), target.to(device)\n",
    "    out, _ = model_example(image, target, prints=True)\n",
    "    loss = criterion_example(out, target)\n",
    "    print('--- LOSS ---', loss.item(), \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --------- GLOBAL PARAMETERS ---------\n",
    "NUM_CLASSES = 15587\n",
    "N_SPLITS = 3\n",
    "BATCH_SIZE = 32\n",
    "MODEL_NAME = 'efficientnet_b0'\n",
    "RUN_NAME = \"B0_neurons_200_embed_200_epochs_4\"\n",
    "EPOCHS = 6\n",
    "VALID_PERC = 0.1\n",
    "NO_NEURONS = 250\n",
    "EMBEDDING_SIZE = 128\n",
    "# -> Optimizer\n",
    "LR = 0.0001\n",
    "WEIGHT_DECAY = 0.000001\n",
    "# -> Scheduler\n",
    "T_MAX = 500              # Maximum number of iterations\n",
    "MIN_LR = 0.000001        # Minimum learning rate. Default: 0\n",
    "# ------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5868"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "del model_example\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_loaders(df, train_i, valid_i):\n",
    "    '''\n",
    "    df: the full initial dataframe\n",
    "    train_i, valid_i: list of indexes for train and validation split\n",
    "    VALID_PERC: percentage of how much of valid data to preserve - leave 1 for full dataset\n",
    "    return: train_loader and valid_loader\n",
    "    '''\n",
    "    \n",
    "    train_df = df.iloc[train_i, :]\n",
    "    # To go quicker through validation\n",
    "    valid_df = df.iloc[valid_i, :].sample(int(len(valid_i)*VALID_PERC), random_state=23)\n",
    "\n",
    "    # Datasets & Dataloader\n",
    "    train_dataset = HappyWhaleDataset(train_df, trainFlag=True)\n",
    "    valid_dataset = HappyWhaleDataset(valid_df, trainFlag=True)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    return train_loader, valid_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_model_optimizer_criterion():\n",
    "    \n",
    "    model = HappyWhaleModel(MODEL_NAME, NUM_CLASSES, NO_NEURONS, EMBEDDING_SIZE).to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY, amsgrad=False)\n",
    "    scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_MAX, eta_min=MIN_LR)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    return model, optimizer, scheduler, criterion\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pipeline(train):\n",
    "    '''\n",
    "    train: the full training dataframe (to be split in train data & valid data)\n",
    "    '''\n",
    "\n",
    "    s = time.time()\n",
    "    \n",
    "    # üêù W&B Initialize\n",
    "    params = dict(model=MODEL_NAME, epochs=EPOCHS, split=N_SPLITS, \n",
    "                  batch=BATCH_SIZE, lr=LR, weight_decay=WEIGHT_DECAY,\n",
    "                  t_max=T_MAX, min_lr=MIN_LR)\n",
    "\n",
    "    \n",
    "    # === CV Split ===\n",
    "    skf = StratifiedKFold(n_splits=N_SPLITS)\n",
    "    skf_splits = skf.split(X=train, y=train[\"individual_key\"])\n",
    "\n",
    "\n",
    "    for fold, (train_i, valid_i) in enumerate(skf_splits):\n",
    "\n",
    "        print(\"~\"*25)\n",
    "        print(\"~\"*8, f\"FOLD {fold}\", \"~\"*8)\n",
    "        print(\"~\"*25)\n",
    "\n",
    "        # Retrieve data loaders\n",
    "        train_loader, valid_loader = get_loaders(train, train_i, valid_i)\n",
    "\n",
    "        # Model/ Optimizer/ Scheduler/ Criterion\n",
    "        model, optimizer, scheduler, criterion = get_model_optimizer_criterion()\n",
    "        # Hooks into the torch model to collect gradients and the topology\n",
    "\n",
    "        # Run Training\n",
    "        BEST_SCORE = 9999\n",
    "\n",
    "        for epoch in range(EPOCHS):\n",
    "            print(\"~\"*8, f\"Epoch {epoch}\", \"~\"*8)\n",
    "\n",
    "            # === TRAIN ===\n",
    "            model.train()\n",
    "            train_losses = []\n",
    "\n",
    "            for images, targets in tqdm(train_loader, desc = 'TRAIN'):\n",
    "                images, targets = images.to(device), targets.to(device)\n",
    "\n",
    "                # Clear gradients BEFORE prediction\n",
    "                optimizer.zero_grad()\n",
    "                # Make predictions\n",
    "                out, _ = model(images, targets)\n",
    "                # Compute Loss and Optimize\n",
    "                loss = criterion(out, targets)             \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_losses.append(loss.cpu().detach().numpy().tolist())\n",
    "\n",
    "            # Adjust Learning Rate\n",
    "            scheduler.step()\n",
    "\n",
    "            mean_train_loss = np.mean(train_losses)\n",
    "            print(\"Mean Train Loss:\", mean_train_loss)\n",
    "\n",
    "\n",
    "            # === EVAL ===\n",
    "            model.eval()\n",
    "            valid_losses, valid_preds, valid_targets = [], [], []\n",
    "            with torch.no_grad():\n",
    "                for images, targets in valid_loader:\n",
    "                    valid_targets.append(targets)\n",
    "                    images, targets = images.to(device), targets.to(device)\n",
    "\n",
    "                    out, _ = model(images, targets)\n",
    "                    loss = criterion(out, targets)\n",
    "\n",
    "                    valid_preds.append(out)\n",
    "                    valid_losses.append(loss.cpu().detach().numpy().tolist())\n",
    "\n",
    "            mean_valid_loss = np.mean(valid_losses)\n",
    "            print(\"Mean Valid Loss:\", mean_valid_loss)\n",
    "            gc.collect()\n",
    "\n",
    "            # === UPDATES ===\n",
    "\n",
    "            if mean_valid_loss < BEST_SCORE:        \n",
    "                print(\"! Saving model in fold {} | epoch {} ...\".format(fold, epoch), \"\\n\")\n",
    "                torch.save(model.state_dict(), f\"EffNetB0_fold_{fold}_loss_{round(mean_valid_loss, 3)}.pt\")\n",
    "\n",
    "                BEST_SCORE = mean_valid_loss\n",
    "\n",
    "        # Clean memory before next fold\n",
    "        del model, optimizer, scheduler, criterion, images, targets, \\\n",
    "                    train_losses, valid_losses, valid_preds, valid_targets\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "    print(f\"Time to run: {round((time.time() - s)/60, 2)} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/efficientnet_b0.ra_in1k)\n",
      "INFO:timm.models._hub:[timm/efficientnet_b0.ra_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~ FOLD 0 ~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~ Epoch 0 ~~~~~~~~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAIN:   6%|‚ñà‚ñà‚ñà‚ñà                                                             | 150/2380 [00:12<03:11, 11.63it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[37], line 42\u001b[0m, in \u001b[0;36mtrain_pipeline\u001b[1;34m(train)\u001b[0m\n\u001b[0;32m     39\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     40\u001b[0m train_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, targets \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader, desc \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTRAIN\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     43\u001b[0m     images, targets \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;66;03m# Clear gradients BEFORE prediction\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[26], line 38\u001b[0m, in \u001b[0;36mHappyWhaleDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     35\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(row\u001b[38;5;241m.\u001b[39mpath)\n\u001b[0;32m     36\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m---> 38\u001b[0m transformed_img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     39\u001b[0m image \u001b[38;5;241m=\u001b[39m transformed_img\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     40\u001b[0m image \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(image)            \n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\albumentations\\core\\composition.py:264\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, force_apply, *args, **data)\u001b[0m\n\u001b[0;32m    261\u001b[0m     p\u001b[38;5;241m.\u001b[39mpreprocess(data)\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m transforms:\n\u001b[1;32m--> 264\u001b[0m     data \u001b[38;5;241m=\u001b[39m t(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdata)\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_each_transform:\n\u001b[0;32m    267\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_data_post_transform(data)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\albumentations\\core\\transforms_interface.py:97\u001b[0m, in \u001b[0;36mBasicTransform.__call__\u001b[1;34m(self, force_apply, *args, **kwargs)\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeterministic:\n\u001b[0;32m     96\u001b[0m         kwargs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_key][\u001b[38;5;28mid\u001b[39m(\u001b[38;5;28mself\u001b[39m)] \u001b[38;5;241m=\u001b[39m deepcopy(params)\n\u001b[1;32m---> 97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_with_params(params, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m kwargs\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\albumentations\\core\\transforms_interface.py:108\u001b[0m, in \u001b[0;36mBasicTransform.apply_with_params\u001b[1;34m(self, params, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_key2func \u001b[38;5;129;01mand\u001b[39;00m arg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    107\u001b[0m     target_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_key2func[key]\n\u001b[1;32m--> 108\u001b[0m     res[key] \u001b[38;5;241m=\u001b[39m target_function(arg, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    110\u001b[0m     res[key] \u001b[38;5;241m=\u001b[39m arg\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\albumentations\\augmentations\\geometric\\transforms.py:1513\u001b[0m, in \u001b[0;36mVerticalFlip.apply\u001b[1;34m(self, img, **params)\u001b[0m\n\u001b[0;32m   1512\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, img: np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m-> 1513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfgeometric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvflip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\albumentations\\augmentations\\geometric\\functional.py:909\u001b[0m, in \u001b[0;36mvflip\u001b[1;34m(img)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvflip\u001b[39m(img: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m--> 909\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mascontiguousarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_pipeline(train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/efficientnet_b0.ra_in1k)\n",
      "INFO:timm.models._hub:[timm/efficientnet_b0.ra_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pretrained_name = \"EffNetB0_fold_0_loss_14.979\"\n",
    "# pretrained_name = \"EffNetB0_fold_1_loss_14.91\"\n",
    "pretrained_name = \"EffNetB0_fold_2_loss_16.367.pt\"\n",
    "\n",
    "# Path to trained model parameters (i.e. weights and biases)\n",
    "classif_model_path = pretrained_name\n",
    "\n",
    "# Load the model and append learned params\n",
    "model = HappyWhaleModel(MODEL_NAME, NUM_CLASSES, NO_NEURONS, EMBEDDING_SIZE).to(device)\n",
    "model.load_state_dict(torch.load(classif_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1785/1785 [03:07<00:00,  9.51it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# DataLoader\n",
    "dataset = HappyWhaleDataset(train, trainFlag=True)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Retrieve all embeddings for each image\n",
    "all_embeddings = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for image, target in tqdm(dataloader): \n",
    "        image, target = image.to(device), target.to(device)\n",
    "        _, embedding = model(image, target)\n",
    "        embedding = embedding.detach().cpu().numpy()\n",
    "        all_embeddings.append(embedding)\n",
    "        \n",
    "# Concatenate batches together\n",
    "image_embeddings = np.concatenate(all_embeddings)\n",
    "\n",
    "# Save embeddings and corresponding image\n",
    "np.save(f'{pretrained_name}.npy', image_embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üêùSave embeddings to W&B\n",
    "save_dataset_artifact(run_name=pretrained_name, \n",
    "                      artifact_name=pretrained_name, \n",
    "                      path=\"../input/happywhale-2022/EffNetB0_fold_0_loss_14.979.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# === CLUSTERING ===\n",
    "# Use the cuml function from RAPIDS suite\n",
    "knn_model = NearestNeighbors(n_neighbors=5)\n",
    "# Train the model\n",
    "knn_model.fit(image_embeddings)\n",
    "\n",
    "# Infer on the training data\n",
    "# distances - the distance between each point in the group\n",
    "# indices - the index row of each image\n",
    "distances, indices = knn_model.kneighbors(image_embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 114213/114213 [00:14<00:00, 7691.04it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# === PREDICTION ===\n",
    "# Create the grouped predictions based on distances & indices\n",
    "predictions = {\"images\": [], \"embeddings\": []}\n",
    "\n",
    "for i in tqdm(range(len(image_embeddings))):\n",
    "    index = np.where(distances[k, ] < 6.0)[0]\n",
    "    split = indices[i, index]\n",
    "    \n",
    "    grouped_images = train.iloc[split][\"image\"].values\n",
    "    grouped_embeddings = image_embeddings[split]\n",
    "\n",
    "    predictions[\"images\"].append(grouped_images)\n",
    "    predictions[\"embeddings\"].append(grouped_embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from seaborn import heatmap\n",
    "\n",
    "# Select a clustered group\n",
    "group = 0\n",
    "\n",
    "example_paths = [\"../input/whale2-cropped-dataset/cropped_train_images/cropped_train_images/\"+img \\\n",
    "                     for img in predictions[\"images\"][group]]\n",
    "example_embeds = predictions[\"embeddings\"][group]\n",
    "\n",
    "# Compute similarity matrix\n",
    "cos_matrix = cosine_similarity(example_embeds)\n",
    "mask = np.zeros_like(cos_matrix)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --------- INFERENCE PARAMETERS ---------\n",
    "PRETRAINED_NAME1 = \"EffNetB0_fold_0_loss_16.666\"\n",
    "PRETRAINED_NAME2 = \"EffNetB0_fold_1_loss_16.539\"\n",
    "PRETRAINED_NAME3 = \"EffNetB0_fold_2_loss_16.367\"\n",
    "MODEL_NAME = 'efficientnet_b0'\n",
    "NUM_CLASSES = 15587\n",
    "NO_NEURONS = 250\n",
    "EMBEDDING_SIZE = 128\n",
    "# ----------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'retrieve_test_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# ===== I. EMBEDDINGS  =====\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Here I am retrieving the 3 test embeddings and averaging them together\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m test_embeddings1 \u001b[38;5;241m=\u001b[39m \u001b[43mretrieve_test_embeddings\u001b[49m(PRETRAINED_NAME1, \n\u001b[0;32m      4\u001b[0m                                             MODEL_NAME, NUM_CLASSES, NO_NEURONS, EMBEDDING_SIZE)\n\u001b[0;32m      5\u001b[0m test_embeddings2 \u001b[38;5;241m=\u001b[39m retrieve_test_embeddings(PRETRAINED_NAME2, \n\u001b[0;32m      6\u001b[0m                                             MODEL_NAME, NUM_CLASSES, NO_NEURONS, EMBEDDING_SIZE)\n\u001b[0;32m      7\u001b[0m test_embeddings3 \u001b[38;5;241m=\u001b[39m retrieve_test_embeddings(PRETRAINED_NAME3, \n\u001b[0;32m      8\u001b[0m                                             MODEL_NAME, NUM_CLASSES, NO_NEURONS, EMBEDDING_SIZE)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'retrieve_test_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ===== I. EMBEDDINGS  =====\n",
    "# Here I am retrieving the 3 test embeddings and averaging them together\n",
    "test_embeddings1 = retrieve_test_embeddings(PRETRAINED_NAME1, \n",
    "                                            MODEL_NAME, NUM_CLASSES, NO_NEURONS, EMBEDDING_SIZE)\n",
    "test_embeddings2 = retrieve_test_embeddings(PRETRAINED_NAME2, \n",
    "                                            MODEL_NAME, NUM_CLASSES, NO_NEURONS, EMBEDDING_SIZE)\n",
    "test_embeddings3 = retrieve_test_embeddings(PRETRAINED_NAME3, \n",
    "                                            MODEL_NAME, NUM_CLASSES, NO_NEURONS, EMBEDDING_SIZE)\n",
    "\n",
    "test_embeddings = (test_embeddings1+test_embeddings2+test_embeddings3)/3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Embeddings: (114213, 128) \n",
      "Train Individual Id: (114213,) \n",
      "\n",
      "Distances shape: (114213, 50) \n",
      "Index shape: (114213, 50)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ===== II. CLUSTERS  =====\n",
    "# Get full train embeddings\n",
    "# Here I am adding the 3 train embeddings and averaging them together as well\n",
    "train_embeddings = np.load(\"./EffNetB0_fold_2_loss_16.367.pt.npy\")\n",
    "\n",
    "train_individual_ids = train[\"individual_id\"].values\n",
    "print(\"Train Embeddings:\", train_embeddings.shape, \"\\n\"+\n",
    "      \"Train Individual Id:\", train_individual_ids.shape, \"\\n\")\n",
    "\n",
    "\n",
    "# Train a final KNN model with the train embeddings\n",
    "knn_final_model = NearestNeighbors(n_neighbors=50)\n",
    "knn_final_model.fit(train_embeddings)\n",
    "\n",
    "# Get distances & indexes for test\n",
    "#test_embeddings = normalize(test_embeddings, axis=1, norm='l2')\n",
    "\n",
    "D, I = knn_final_model.kneighbors(train_embeddings)\n",
    "print(\"Distances shape:\", D.shape, \"\\n\"+\n",
    "      \"Index shape:\", I.shape)\n",
    "\n",
    "# List of the test dataframe image ids (to loop through it)\n",
    "test_images = test[\"image\"].tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51033it [00:10, 4827.90it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "test_df = []\n",
    "\n",
    "# Loop through each observation within test data\n",
    "for k, image_id in tqdm(enumerate(test_images)):\n",
    "    # Get individual_id & distances for the observation\n",
    "    individual_id = train_individual_ids[I[k]]\n",
    "    distances = D[k]\n",
    "    # Create a df subset with this info\n",
    "    subset_preds = pd.DataFrame(np.stack([individual_id, distances], axis=1),\n",
    "                                columns=['individual_id','distances'])\n",
    "    subset_preds['image_id'] = image_id\n",
    "    test_df.append(subset_preds)\n",
    "    \n",
    "    \n",
    "# Concatenate subset dataframes into 1 dataframe\n",
    "test_df = pd.concat(test_df).reset_index(drop=True)\n",
    "# Choose max distance for each unique pair of individual_id & image_id\n",
    "test_df = test_df.groupby(['image_id','individual_id'])['distances'].max().reset_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>individual_id</th>\n",
       "      <th>distances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2309569</th>\n",
       "      <td>ebf163f8ea6876.jpg</td>\n",
       "      <td>36a2b0f9281f</td>\n",
       "      <td>0.00953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1923562</th>\n",
       "      <td>c48f1de1806053.jpg</td>\n",
       "      <td>2393ace8cd64</td>\n",
       "      <td>0.004864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168397</th>\n",
       "      <td>115b95e01978e1.jpg</td>\n",
       "      <td>8ce3b57bc545</td>\n",
       "      <td>0.004645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1756395</th>\n",
       "      <td>b359dcd50304e5.jpg</td>\n",
       "      <td>86257eaa613b</td>\n",
       "      <td>0.00946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1656220</th>\n",
       "      <td>a9466ee34493e6.jpg</td>\n",
       "      <td>c493f38f5e18</td>\n",
       "      <td>0.003214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   image_id individual_id distances\n",
       "2309569  ebf163f8ea6876.jpg  36a2b0f9281f   0.00953\n",
       "1923562  c48f1de1806053.jpg  2393ace8cd64  0.004864\n",
       "168397   115b95e01978e1.jpg  8ce3b57bc545  0.004645\n",
       "1756395  b359dcd50304e5.jpg  86257eaa613b   0.00946\n",
       "1656220  a9466ee34493e6.jpg  c493f38f5e18  0.003214"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Have a look at the predictions dataset now\n",
    "test_df.sample(n=5, random_state=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2510053it [01:18, 31862.51it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51033/51033 [00:00<00:00, 3904843.77it/s]\n"
     ]
    }
   ],
   "source": [
    "# ===== III. PREDICTION  =====\n",
    "\n",
    "# Dictionary in format: {[\"image_id\"]: 000, [\"individual_id\"]: 9999}\n",
    "predictions = {}\n",
    "thresh = 5\n",
    "\n",
    "for k, row in tqdm(test_df.iterrows()):\n",
    "    image_id = row[\"image_id\"]\n",
    "    individual_id = row[\"individual_id\"]\n",
    "    distance = row[\"distances\"]\n",
    "    \n",
    "    # If the image_id has already been added in predictions before\n",
    "    if image_id in predictions:\n",
    "        # If total preds for this image_id are < 5 then add, else continue\n",
    "        if len(predictions[image_id]) != 5:\n",
    "            predictions[image_id].append(individual_id)\n",
    "        else:\n",
    "            continue\n",
    "    # If the distance is greater than thresh add prediction + \"new_individual\"\n",
    "    elif distance > thresh:\n",
    "        predictions[image_id] = [individual_id, \"new_individual\"]\n",
    "    else:\n",
    "        predictions[image_id] = [\"new_individual\", individual_id]\n",
    "\n",
    "\n",
    "# Fill in all lists that have less than 5 predictions as of yet\n",
    "sample_list = ['37c7aba965a5', '114207cab555', 'a6e325d8e924', '19fbb960f07d','c995c043c353']\n",
    "\n",
    "for image_id, preds in tqdm(predictions.items()):\n",
    "    if len(preds) < 5:\n",
    "        remaining = [individ_id for individ_id in sample_list if individ_id not in preds]\n",
    "        preds.extend(remaining)\n",
    "        predictions[image_id] = preds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00021adfb725ed.jpg</td>\n",
       "      <td>new_individual 07a477b6a091 0f302e71d455 1394c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000562241d384d.jpg</td>\n",
       "      <td>new_individual 02f9cc951294 04687f521fdf 0857f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0007c33415ce37.jpg</td>\n",
       "      <td>new_individual 01db941ec9b6 044434ed1926 101e1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0007d9bca26a99.jpg</td>\n",
       "      <td>new_individual 005cab4fa315 033a294ca772 063ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00087baf5cef7a.jpg</td>\n",
       "      <td>new_individual 0175ed3e02a4 04b434e53a13 06165...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                image                                        predictions\n",
       "0  00021adfb725ed.jpg  new_individual 07a477b6a091 0f302e71d455 1394c...\n",
       "1  000562241d384d.jpg  new_individual 02f9cc951294 04687f521fdf 0857f...\n",
       "2  0007c33415ce37.jpg  new_individual 01db941ec9b6 044434ed1926 101e1...\n",
       "3  0007d9bca26a99.jpg  new_individual 005cab4fa315 033a294ca772 063ca...\n",
       "4  00087baf5cef7a.jpg  new_individual 0175ed3e02a4 04b434e53a13 06165..."
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create final submission\n",
    "predictions = pd.Series(predictions).reset_index()\n",
    "predictions.columns = ['image','predictions']\n",
    "predictions['predictions'] = predictions['predictions'].apply(lambda x: ' '.join(x))\n",
    "predictions.to_csv('submission.csv',index=False)\n",
    "\n",
    "predictions.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
